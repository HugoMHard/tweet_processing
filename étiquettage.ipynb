{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MDXxTvsFJ1X"
      },
      "source": [
        "## Importations des données et modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33G8T1b4x6mg",
        "outputId": "4941d79a-f859-4010-f6d2-8c2e56b08861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fr-core-news-md==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.7.0/fr_core_news_md-3.7.0-py3-none-any.whl (45.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-md==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (0.1.2)\n",
            "Installing collected packages: fr-core-news-md\n",
            "Successfully installed fr-core-news-md-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "import string\n",
        "import re\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.util import ngrams\n",
        "# Téléchargement du modèle spaCy en ligne de commande (à exécuter une seule fois)\n",
        "!python -m spacy download fr_core_news_md\n",
        "nlp = spacy.load('fr_core_news_md')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh9etHBDxraQ"
      },
      "outputs": [],
      "source": [
        "# On importe les tweetsen csv\n",
        "\n",
        "bdd_L_link = \"/content/drive/MyDrive/projet/Tweet2.csv\"\n",
        "bdd_D_link = \"/content/drive/MyDrive/projet/Tweet1.csv\"\n",
        "Tweet_csv_L = pd.read_csv(bdd_L_link, on_bad_lines='skip', sep=';')\n",
        "Tweet_csv_D = pd.read_csv(bdd_D_link, on_bad_lines='skip',sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhEoJebsyxCQ"
      },
      "outputs": [],
      "source": [
        "# On réindexe et supprimme les lignes\n",
        "\n",
        "Tweet_csv_L=Tweet_csv_L[['data__id','data__text']]\n",
        "Tweet_csv_D=Tweet_csv_D[['data__id','data__text']]\n",
        "\n",
        "Tweet_csv_L=(Tweet_csv_L.dropna(subset='data__text')).reset_index(drop=False)\n",
        "Tweet_csv_D=(Tweet_csv_D.dropna(subset='data__text')).reset_index(drop=False)\n",
        "Tweet_csv_L=(Tweet_csv_L[Tweet_csv_L['data__text']!='data__text'])[['index','data__text']]\n",
        "Tweet_csv_D=(Tweet_csv_D[Tweet_csv_D['data__text']!='data__text'])[['index','data__text']]\n",
        "#print(Tweet_csv_D.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fT7o0Zk6HOaw"
      },
      "outputs": [],
      "source": [
        "# On créé un data set d'entraînemnt et un autre de test\n",
        "\n",
        "Tweet_csv_L_train, Tweet_csv_L_test = train_test_split(Tweet_csv_L, test_size=0.2, random_state=42)\n",
        "Tweet_csv_D_train, Tweet_csv_D_test = train_test_split(Tweet_csv_D, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYpwQFlUFXMj"
      },
      "source": [
        "# Nettoyage des données (url, ponctuation, lemmatisation, ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7Hx0qcR4lBb"
      },
      "outputs": [],
      "source": [
        "# On créer des dictionnaire contenant des dictionnaires sur les tweets\n",
        "\n",
        "def concatenate_dictionaries(dict1, dict2):\n",
        "  \"\"\"Concatenates two dictionaries with incrementing indices.\n",
        "\n",
        "  Args:\n",
        "    dict1: The first dictionary.\n",
        "    dict2: The second dictionary.\n",
        "\n",
        "  Returns:\n",
        "    A new dictionary containing the elements of both input dictionaries\n",
        "    with incrementing indices.\n",
        "  \"\"\"\n",
        "\n",
        "  max_key_dict1 = 0\n",
        "  if dict1:  # Check if dict1 is not empty\n",
        "    max_key_dict1 = max(int(key) for key in dict1.keys())\n",
        "\n",
        "  result_dict = dict1.copy()  # Start with a copy of dict1\n",
        "\n",
        "  for key, value in dict2.items():\n",
        "    result_dict[str(int(key) + max_key_dict1 + 1)] = value\n",
        "    # Increment key and add to result_dict\n",
        "\n",
        "  return result_dict\n",
        "\n",
        "def create_dict(Tweet_csv,dict_to_complete):\n",
        "  \"\"\" create_dict créé un dictionnaire à partir d'un csv. Chaque clef est un index se référant à un tweet.\n",
        "      La valeur du dictionnaire en cet index est un dictionnaire concernant le tweet\n",
        "    :param create_dict: csv contenant les tweets, dictionnaire qui contiendra l'information sur le tweets\n",
        "    :return: dictionnaire contenant les tweets\n",
        "  \"\"\"\n",
        "  for index, row in Tweet_csv.iterrows():  # Iterate using iterrows()\n",
        "    tweet_text = row['data__text']  # Directly access the text as a string\n",
        "    dict_to_complete[index] = {'text': tweet_text}\n",
        "  return\n",
        "\n",
        "dict_combine, dict_L, dict_D = {}, {}, {}\n",
        "dict_train_L, dict_train_D = {}, {}\n",
        "dict_test_L, dict_test_D = {}, {}\n",
        "\n",
        "create_dict(Tweet_csv_L,dict_L)\n",
        "create_dict(Tweet_csv_D,dict_D)\n",
        "create_dict(Tweet_csv_L_train,dict_train_L)\n",
        "create_dict(Tweet_csv_D_train,dict_train_D)\n",
        "create_dict(Tweet_csv_L_test,dict_test_L)\n",
        "create_dict(Tweet_csv_D_test,dict_test_D)\n",
        "dict_combine = concatenate_dictionaries(dict_L, dict_D)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dict_combine), len(dict_L),len(dict_D))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHSwWhSQm-vz",
        "outputId": "4539badb-afe9-4ab7-fe0b-c278b7082536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17076 5636 11440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oID1tG1O5Ahg"
      },
      "outputs": [],
      "source": [
        "url_pattern = re.compile(r'(https?://|www\\.)\\S+')\n",
        "pattern=r'\\w+|#[a-zA-Z0-9_àâäéèêëïîôöùûüçÀÂÄÉÈÊËÏÎÔÖÙÛÜÇ]+|@[a-zA-Z0-9_àâäéèêëïîôöùûüçÀÂÄÉÈÊËÏÎÔÖÙÛÜÇ]+|[^\\w\\s]+'\n",
        "tokenizer = RegexpTokenizer(pattern)\n",
        "\n",
        "def remove_urls(text):\n",
        "    return url_pattern.sub('', text)\n",
        "\n",
        "def remove_punctuation(text_without_urls):\n",
        "  \"\"\" Fonction pour retirer la ponctuation des tweets, les stop_words et les tokeniser.\n",
        "    :param remove_punctuation: String contenant le tweet\n",
        "    :return: Liste des tokens\n",
        "  \"\"\"\n",
        "  global dict_combine\n",
        "  without_punctuation = []\n",
        "  text_tokenise = tokenizer.tokenize(text_without_urls)\n",
        "  for word in text_tokenise:\n",
        "    if word.isalpha() or (word.startswith('#') and len(word_tokenize(word))>1) or (word.startswith('@') and len(word_tokenize(word))>1):\n",
        "    # We import and use the stopwords from NLTK library\n",
        "      if word.lower() not in stopwords.words('french'):\n",
        "        without_punctuation.append(word.lower())\n",
        "  return without_punctuation\n",
        "\n",
        "def ajt_tokenize (dictio):\n",
        "  for key in dictio.keys():\n",
        "    text=dictio[key]['text']\n",
        "    text_without_urls = remove_urls(text)\n",
        "    dictio[key]['text_tokenise'] = remove_punctuation(text_without_urls)\n",
        "  return\n",
        "\n",
        "ajt_tokenize(dict_combine)\n",
        "ajt_tokenize(dict_test_L)\n",
        "ajt_tokenize(dict_test_D)\n",
        "ajt_tokenize(dict_train_L)\n",
        "ajt_tokenize(dict_train_D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74iZbKIB_eWC",
        "outputId": "c8963410-7df7-4254-b200-99e6624136bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \"RT @LutteOuvriere: #Migrants, #chômage, #police : une société en crise, malade du capitalisme \\\\n L'édito ⬇️ \\\\n https://t.co/sxkwJdMZXX\", 'text_tokenise': ['rt', '@lutteouvriere', '#migrants', '#chômage', '#police', 'société', 'crise', 'malade', 'capitalisme', 'édito']}\n"
          ]
        }
      ],
      "source": [
        "# Pour s'assurer que 'text_tokenise\" ne contient pas d'url, ponctuation,etc.\n",
        "print(dict_combine[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9p7_WK_0iPx"
      },
      "outputs": [],
      "source": [
        "def lemmatize_tokens_g(phrase_tokenisee):\n",
        "    \"\"\"\n",
        "    Fonction pour lemmatiser les tokens d'une phrase tokenisée.\n",
        "\n",
        "    :param phrase_tokenisee_g : Objet spaCy Doc contenant les tokens\n",
        "    :return: Liste des mots lemmatisés\n",
        "    \"\"\"\n",
        "    mots_lemmatises = []\n",
        "    global mots_lem_g  # Accès à la liste globale pour stocker tous les mots lemmatisés\n",
        "\n",
        "    j = 0\n",
        "    while j < len(phrase_tokenisee):\n",
        "        token = phrase_tokenisee[j]\n",
        "\n",
        "        if token.text.startswith('#') and j + 1 < len(phrase_tokenisee):\n",
        "            # Combinaison du hashtag avec le token suivant\n",
        "            combined_text = token.text + phrase_tokenisee[j + 1].text\n",
        "            combined_token = nlp(combined_text)\n",
        "            mots_lemmatises.append(combined_token)\n",
        "            mots_lem_g.append(combined_token)\n",
        "            j += 2  # Passer au token après le suivant\n",
        "        elif token.text.startswith('@'):\n",
        "            # Lemmatisation des mentions\n",
        "            mots_lemmatises.append(token.lemma_)\n",
        "            mots_lem_g.append(token.lemma_)\n",
        "            j += 1\n",
        "        else :\n",
        "            # Lemmatisation des autres mots\n",
        "            mots_lemmatises.append(token.lemma_)\n",
        "            mots_lem_g.append(token.lemma_)\n",
        "            j += 1\n",
        "\n",
        "    return mots_lemmatises\n",
        "\n",
        "def lemmatize_tokens(phrase_tokenisee):\n",
        "    \"\"\"\n",
        "    Fonction pour lemmatiser les tokens d'une phrase tokenisée.\n",
        "\n",
        "    :param phrase_tokenisee: Objet spaCy Doc contenant les tokens\n",
        "    :return: Liste des mots lemmatisés\n",
        "    \"\"\"\n",
        "    mots_lemmatises = []\n",
        "    global mots_lem_g  # Accès à la liste globale pour stocker tous les mots lemmatisés\n",
        "\n",
        "    j = 0\n",
        "    while j < len(phrase_tokenisee):\n",
        "        token = phrase_tokenisee[j]\n",
        "\n",
        "        if token.text.startswith('#') and j + 1 < len(phrase_tokenisee):\n",
        "            # Combinaison du hashtag avec le token suivant\n",
        "            combined_text = token.text + phrase_tokenisee[j + 1].text\n",
        "            combined_token = nlp(combined_text)\n",
        "            mots_lemmatises.append(combined_token)\n",
        "            j += 2  # Passer au token après le suivant\n",
        "        elif token.text.startswith('@'):\n",
        "            # Lemmatisation des mentions\n",
        "            mots_lemmatises.append(token.lemma_)\n",
        "            j += 1\n",
        "        else :\n",
        "            # Lemmatisation des autres mots\n",
        "            mots_lemmatises.append(token.lemma_)\n",
        "            j += 1\n",
        "\n",
        "    return mots_lemmatises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aatI_J1k3-NW",
        "outputId": "3e213700-ef7d-4101-fc13-6cd6e11f45c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fait 1\n",
            "fait 2\n",
            "fait 3\n",
            "fait 4\n"
          ]
        }
      ],
      "source": [
        "# Liste globale pour stocker tous les mots lemmatisés, t exécution\n",
        "mots_lem_g = []\n",
        "\n",
        "def lemmatisation (dict_ou_ajouter):\n",
        "  for index, value in dict_ou_ajouter.items():\n",
        "    # Création de l'objet spaCy à partir de la liste 'text_tokenise'\n",
        "    phrase_tokenisee = nlp(\" \".join(value['text_tokenise']))\n",
        "    # Appel de la fonction pour lemmatiser les tokens\n",
        "    value['mots_lemmatises'] = lemmatize_tokens(phrase_tokenisee)\n",
        "  return\n",
        "\n",
        "def lemmatisation_g (dict_ou_ajouter):\n",
        "  for index, value in dict_ou_ajouter.items():\n",
        "    # Création de l'objet spaCy à partir de la liste 'text_tokenise\n",
        "    phrase_tokenisee = nlp(\" \".join(value['text_tokenise']))\n",
        "    # Appel de la fonction pour lemmatiser les tokens\n",
        "    value['mots_lemmatises'] = lemmatize_tokens_g(phrase_tokenisee)\n",
        "  return\n",
        "\n",
        "lemmatisation_g(dict_combine)\n",
        "print(\"fait 1\")\n",
        "lemmatisation(dict_test_L)\n",
        "print(\"fait 2\")\n",
        "lemmatisation(dict_test_D)\n",
        "print(\"fait 3\")\n",
        "lemmatisation(dict_train_L)\n",
        "print(\"fait 4\")\n",
        "lemmatisation(dict_train_D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FV2umtyIBga-",
        "outputId": "577916d7-1155-4156-e6bf-3b34fbe72877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['principaux', 'candidats', 'gauche', 'droite', 'choisi', 'faire', 'terrorisme', 'migrants', 'centre', 'campagne', '#lt']\n",
            "['principal', 'candidat', 'gauche', 'droite', 'choisir', 'faire', 'terrorisme', 'migrant', 'centre', 'campagne', #lt]\n"
          ]
        }
      ],
      "source": [
        "# On vérifie que les mots sont biens lemmatisés\n",
        "i=4\n",
        "print(dict_combine[i]['text_tokenise'])\n",
        "print(dict_combine[i]['mots_lemmatises'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUSMRh_PaDLU",
        "outputId": "e8b14209-8d30-4c67-b69d-ecbca9c2a4b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('rt', '@nicolasbay', '_'), 152), (('loi', 'asil', 'immigration'), 107), (('lutte', 'contre', 'immigration'), 87), (('lutter', 'contre', 'immigration'), 83), (('pompe', 'aspirant', 'immigration'), 73), (('contre', 'immigration', 'clandestin'), 71), (('traiter', 'libre', 'échange'), 67), (('mettre', 'fin', 'immigration'), 64), (('lien', 'entrer', 'immigration'), 58), (('emmanuel', 'macron', 'avoir'), 57), (('ce', 'immigration', 'massif'), 56), (('trafic', 'être', 'humain'), 54), (('politique', 'dissuasif', 'immigration'), 48), (('contre', 'submersion', 'migratoire'), 43), (('mettre', 'place', 'politique'), 42), (('contre', 'immigration', 'massif'), 41), (('mlp', 'rt', '@mlp_officiel'), 40), (('toujours', 'plus', 'immigration'), 39), (('rt', '@mlp_officiel', 'immigration'), 39), (('rt', '@mlp_officiel', 'avoir'), 37), (('@cnews', 'rt', '@mlp_officiel'), 36), (('projet', 'loi', 'asil'), 35), (('contre', 'immigration', 'illégal'), 35), (('rt', '@dlf_officiel', 'dupontaignan'), 35), (('français', 'vouloir', 'plus'), 34), (('rt', '@mlp_officiel', 'falloir'), 34), (('fin', 'immigration', 'massif'), 34), (('français', 'pouvoir', 'plus'), 34), (('débouter', 'droit', 'asil'), 33), (('falloir', 'arrêter', 'immigration'), 33), (('ce', 'submersion', 'migratoire'), 33), (('place', 'politique', 'dissuasif'), 33), (('face', 'immigration', 'massif'), 33), (('politique', 'immigration', 'massif'), 32), (('submersion', 'migratoire', 'pays'), 32), (('couper', 'pompe', 'aspirant'), 31), (('@lci', 'rt', '@mlp_officiel'), 30), (('union', 'européen', 'avoir'), 29), (('liberté', 'circulation', 'installation'), 28), (('immigration', 'massif', 'avoir'), 28), (('organiser', 'submersion', 'migratoire'), 28), (('face', 'submersion', 'migratoire'), 28), (('milliard', 'euro', 'an'), 27), (('falloir', 'mettre', 'fin'), 27), (('peser', 'baisse', 'salaire'), 27), (('rétablir', 'frontière', 'national'), 27), (('million', 'chômeur', 'million'), 27), (('chômeur', 'million', 'pauvre'), 27), (('retrouver', 'frontière', 'national'), 27), (('ong', 'complice', 'passeur'), 26), (('projet', 'loi', 'immigration'), 26), (('supprimer', 'droit', 'sol'), 26), (('immigration', 'massif', 'anarchique'), 26), (('régler', 'problème', 'immigration'), 26), (('rt', '@mlp_officiel', 'vouloir'), 26), (('politique', 'anti', 'migrant'), 25), (('falloir', 'mettre', 'place'), 25), (('fou', 'politique', 'migratoire'), 25), (('centre', 'rétention', 'administratif'), 24), (('entrer', 'immigration', 'insécurité'), 24), (('mettre', 'fin', 'ce'), 24), (('pacte', 'mondial', 'migration'), 24), (('aide', 'médical', 'etat'), 24), (('zone', 'non', 'droit'), 23), (('accueil', 'digne', 'migrant'), 23), (('ce', 'après', 'midi'), 23), (('arrêter', 'immigration', 'massif'), 23), (('@cnews', 'rt', '@j_bardella'), 23), (('suppression', 'droit', 'sol'), 23), (('politique', 'migratoire', 'gouvernement'), 22), (('politique', 'migratoire', 'européen'), 22), (('seine', 'saint', 'denis'), 22), (('immigration', 'fraude', 'social'), 22), (('@bfmtv', 'rt', '@mlp_officiel'), 22), (('vouloir', 'plus', 'payer'), 22), (('signer', 'pétition', 'rt'), 21), (('tout', 'pompe', 'aspirant'), 21), (('appel', 'air', 'migratoire'), 21), (('parlement', 'européen', 'voter'), 21), (('vouloir', 'plus', 'immigration'), 21), (('lutt', 'contre', 'immigration'), 20), (('avoir', 'lien', 'entrer'), 20), (('communiqué', 'presse', 'rt'), 20), (('créer', 'appel', 'air'), 20), (('@cnews', 'rt', '@fn_officiel'), 20), (('maîtrise', 'flux', 'migratoire'), 20), (('ce', 'union', 'européen'), 20), (('@bfmtv', 'rt', '@rnational_off'), 20), (('@bfmtv', 'rt', '@j_bardella'), 20), (('entrer', 'immigration', 'terrorisme'), 19), (('contre', 'immigration', 'irrégulier'), 19), (('lien', 'évident', 'entrer'), 19), (('battre', 'tout', 'record'), 19), (('devant', 'assemblée', 'national'), 18), (('emmanuel', 'macron', 'vouloir'), 18), (('face', 'crise', 'migratoire'), 18), (('mineur', 'non', 'accompagner'), 18), (('immigration', 'massif', 'incontrôlé'), 18), (('voter', 'parlement', 'européen'), 18), (('pouvoir', 'plus', 'accueillir'), 18)]\n"
          ]
        }
      ],
      "source": [
        "#afficher les k n-grammes les plus communs dans le texte, ici les 100 premiers trigrammes\n",
        "n=3\n",
        "k=100\n",
        "ngram = list(nltk.ngrams(mots_lem_g,n))\n",
        "fdist = FreqDist(ngram)\n",
        "print(fdist.most_common(k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTRmZjK0Cwzj",
        "outputId": "2d23210e-71a5-47e8-c517-a3d0b1dab5da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size before lemmatization: 23713\n",
            "Vocabulary size after lemmatization: 35668\n"
          ]
        }
      ],
      "source": [
        "# Calculate vocabulary size before lemmatization\n",
        "vocab_before_lemmatization = set()\n",
        "for key in dict_combine:\n",
        "    vocab_before_lemmatization.update(dict_combine[key]['text_tokenise'])\n",
        "print(f\"Vocabulary size before lemmatization: {len(vocab_before_lemmatization)}\")\n",
        "\n",
        "# Calculate vocabulary size after lemmatization\n",
        "vocab_after_lemmatization = set(mots_lem_g)\n",
        "print(f\"Vocabulary size after lemmatization: {len(vocab_after_lemmatization)}\")\n",
        "\n",
        "# ... (rest of your code)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pTn_a1IQk76y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpRWL1eMGGSK"
      },
      "source": [
        "# Création du vocabulaire finale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmYpuUg1DaJF",
        "outputId": "5463f7f7-09f6-40ea-c586-7a9b288061f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size after lemmatization: 438867\n"
          ]
        }
      ],
      "source": [
        "les_n = [2,3]\n",
        "for i in les_n:\n",
        "  n=i\n",
        "  ngram = list(nltk.ngrams(mots_lem_g,n))\n",
        "  vocab_after_lemmatization.update(ngram)\n",
        "print(f\"Vocabulary size after lemmatization: {len(vocab_after_lemmatization)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOEX6CekGder"
      },
      "source": [
        "## Ressources (normale si ça plante  ici)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "GvEAlr_B7mqY",
        "outputId": "9ba80ca2-d68f-4d8c-d024-7056361428f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'iloc'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-6bb0ce6485dc>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict_combine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Access the tweet text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'iloc'"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for key, value in dict_combine.items():\n",
        "    tweet = value['text'].iloc[0]  # Access the tweet text\n",
        "    tokens = word_tokenize(tweet)\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    value['lemmas'] = lemmas # Add the lemmas to the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN0zcFl8mIA_"
      },
      "outputs": [],
      "source": [
        "#!python -m spacy download fr_core_news_md\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk import FreqDist\n",
        "#spacy.load('fr_core_news_md') # No need to call spacy.load twice\n",
        "# On va lématiser les mots\n",
        "nlp = spacy.load('fr_core_news_md')\n",
        "mots_lem_g=[]\n",
        "for index in dict_combine.keys():\n",
        "  mots_lemmatises=[]\n",
        "  # Accessing the correct key 'text_tokenise'\n",
        "  phrase_de_mots_tokenise=nlp(\" \".join(dict_combine[index]['text_tokenise']))\n",
        "  contenant = {}\n",
        "  for i,w in enumerate(phrase_de_mots_tokenise):\n",
        "    contenant[i]=w\n",
        "    j=0\n",
        "  while (j)!=len(phrase_de_mots_tokenise):\n",
        "    w=contenant[j]\n",
        "    if  w.text.startswith('#') and (j+1!=len(phrase_de_mots_tokenise)):\n",
        "      w=w.text+phrase_de_mots_tokenise[j+1].text\n",
        "      w=nlp(w)  # Get the first token from the processed text\n",
        "      mots_lemmatises.append(w) # Append lemma of combined token\n",
        "      mots_lem_g.append(w)\n",
        "      j+=2\n",
        "    elif w.text.startswith('@') and (j+1!=len(phrase_de_mots_tokenise)):\n",
        "      mots_lemmatises.append(w.lemma_)\n",
        "      mots_lem_g.append(w.lemma_)\n",
        "      j+=1\n",
        "    else:\n",
        "      mots_lemmatises.append(w.lemma_)\n",
        "      mots_lem_g.append(w.lemma_)\n",
        "      j+=1\n",
        "\n",
        "  dict_combine[index]['mots_lemmatises']=mots_lemmatises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCxggiNB-Efp"
      },
      "outputs": [],
      "source": [
        "url_pattern = re.compile(r'(https?://|www\\.)\\S+')\n",
        "pattern=r'\\w+|#[a-zA-Z0-9_àâäéèêëïîôöùûüçÀÂÄÉÈÊËÏÎÔÖÙÛÜÇ]+|@[a-zA-Z0-9_àâäéèêëïîôöùûüçÀÂÄÉÈÊËÏÎÔÖÙÛÜÇ]+|[^\\w\\s]+'\n",
        "tokenizer = RegexpTokenizer(pattern)\n",
        "for key in dict_combine.keys():\n",
        "\n",
        "  text=dict_combine[key]['text']\n",
        "  text_without_urls = url_pattern.sub('url', text) # On enlève les urls du code\n",
        "\n",
        "  # Personnalisation du tokenizer pour garder les # ou @ avec le mot\n",
        "\n",
        "  text_tokenise = tokenizer.tokenize(text_without_urls)\n",
        "  without_punctuation = []\n",
        "\n",
        "  # On tokenize en enlevant les stop_words en gardant les mentions et tweets\n",
        "  for word in text_tokenise:\n",
        "    if word.isalpha() or (word.startswith('#') and len(word_tokenize(word))>1) or (word.startswith('@') and len(word_tokenize(word))>1):\n",
        "      if word.lower() not in stopwords.words('french'):\n",
        "        without_punctuation.append(word.lower())\n",
        "  dict_combine[key]['text_tokenise'] = without_punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFdbfwbxYdb8"
      },
      "outputs": [],
      "source": [
        "#!python -m spacy download fr_core_news_md\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk import FreqDist\n",
        "#spacy.load('fr_core_news_md') # No need to call spacy.load twice\n",
        "# On va lématiser les mots\n",
        "nlp = spacy.load('fr_core_news_md')\n",
        "mots_lem_g=[]\n",
        "for index in dict_combine.keys():\n",
        "  mots_lemmatises=[]\n",
        "  # Accessing the correct key 'text_tokenise'\n",
        "  phrase_de_mots_tokenise=nlp(\" \".join(dict_combine[index]['text_tokenise']))\n",
        "  for i,w in enumerate(phrase_de_mots_tokenise):\n",
        "    if (w.text.startswith('@') or w.text.startswith('#')) and (i+1!=len(phrase_de_mots_tokenise)):\n",
        "      w=w.text+phrase_de_mots_tokenise[i+1].text\n",
        "      combined_token = nlp(w)[0]  # Get the first token from the processed text\n",
        "      mots_lemmatises.append(combined_token.lemma_) # Append lemma of combined token\n",
        "      mots_lem_g.append(combined_token.lemma_)\n",
        "    else:\n",
        "      mots_lemmatises.append(w.lemma_)\n",
        "      mots_lem_g.append(w.lemma_)\n",
        "  dict_combine[index]['mots_lemmatises']=mots_lemmatises"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}