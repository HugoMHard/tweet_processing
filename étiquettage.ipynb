{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqFCSOV44zVEv04B1gSt5i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HugoMHard/tweet_processing/blob/main/%C3%A9tiquettage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "import string\n",
        "import re\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.util import ngrams\n",
        "# Téléchargement du modèle spaCy en ligne de commande (à exécuter une seule fois)\n",
        "#!python -m spacy download fr_core_news_md\n",
        "nlp = spacy.load('fr_core_news_md')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33G8T1b4x6mg",
        "outputId": "3c61fe50-2217-4cc2-c2f1-ed21605e5431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh9etHBDxraQ"
      },
      "outputs": [],
      "source": [
        "# On importe les tweetsen csv\n",
        "\n",
        "bdd_L_link = \"/content/drive/MyDrive/projet/Tweet2.csv\"\n",
        "bdd_D_link = \"/content/drive/MyDrive/projet/Tweet1.csv\"\n",
        "Tweet_csv_L = pd.read_csv(bdd_L_link, on_bad_lines='skip', sep=';')\n",
        "Tweet_csv_D = pd.read_csv(bdd_D_link, on_bad_lines='skip',sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# On réindexe et supprimme les lignes\n",
        "\n",
        "Tweet_csv_L=Tweet_csv_L[['data__id','data__text']]\n",
        "Tweet_csv_D=Tweet_csv_D[['data__id','data__text']]\n",
        "\n",
        "Tweet_csv_L=(Tweet_csv_L.dropna(subset='data__text')).reset_index(drop=False)\n",
        "Tweet_csv_D=(Tweet_csv_D.dropna(subset='data__text')).reset_index(drop=False)\n",
        "Tweet_csv_L=(Tweet_csv_L[Tweet_csv_L['data__text']!='data__text'])[['index','data__text']]\n",
        "Tweet_csv_D=(Tweet_csv_D[Tweet_csv_D['data__text']!='data__text'])[['index','data__text']]\n",
        "#print(Tweet_csv_D.head(10))"
      ],
      "metadata": {
        "id": "rhEoJebsyxCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On créer des dictionnaire contenant des dictionnaires sur les tweets\n",
        "\n",
        "dict_combine, dict_L, dict_D = {}, {}, {}\n",
        "l = len(Tweet_csv_L)\n",
        "\n",
        "for index, row in Tweet_csv_L.iterrows():  # Iterate using iterrows()\n",
        "  reset_index = row['index']\n",
        "  tweet_text = row['data__text']  # Directly access the text as a string\n",
        "  dict_combine[reset_index] = {'text': tweet_text}\n",
        "  dict_L[reset_index] = {'text': tweet_text}\n",
        "\n",
        "for index, row in Tweet_csv_D.iterrows():  # Iterate using iterrows()\n",
        "  tweet_text = row['data__text']  # Directly access the text as a string\n",
        "  dict_combine[reset_index + l -1] = {'text': tweet_text} # Updated index for combine\n",
        "  dict_D[index] = {'text': tweet_text}"
      ],
      "metadata": {
        "id": "g7Hx0qcR4lBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On ajoute au dico le texte tokeniser et trier des stop_words"
      ],
      "metadata": {
        "id": "S0oi2Nn3ERfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_pattern = re.compile(r'(https?://|www\\.)\\S+')\n",
        "pattern=r'\\w+|#[a-zA-Z0-9_àâäéèêëïîôöùûüçÀÂÄÉÈÊËÏÎÔÖÙÛÜÇ]+|@[a-zA-Z0-9_àâäéèêëïîôöùûüçÀÂÄÉÈÊËÏÎÔÖÙÛÜÇ]+|[^\\w\\s]+'\n",
        "tokenizer = RegexpTokenizer(pattern)\n",
        "\n",
        "def remove_urls(text):\n",
        "    return url_pattern.sub('url', text)\n",
        "\n",
        "def remove_punctuation(text_without_urls):\n",
        "    global dict_combine\n",
        "    without_punctuation = []\n",
        "    text_tokenise = tokenizer.tokenize(text_without_urls)\n",
        "    for word in text_tokenise:\n",
        "      if word.isalpha() or (word.startswith('#') and len(word_tokenize(word))>1) or (word.startswith('@') and len(word_tokenize(word))>1):\n",
        "      # We import and use the stopwords from NLTK library\n",
        "        if word.lower() not in stopwords.words('french'):\n",
        "          without_punctuation.append(word.lower())\n",
        "    dict_combine[key]['text_tokenise'] = without_punctuation\n",
        "    return without\n",
        "\n",
        "for key in dict_combine.keys():\n",
        "\n",
        "  text=dict_combine[key]['text']\n",
        "  text_without_urls = remove_urls(text)\n",
        "\n",
        "  # Personnalisation du tokenizer pour garder les # ou @ avec le mot\n",
        "\n",
        "  text_tokenise = tokenizer.tokenize(text_without_urls)\n",
        "  without_punctuation = []\n",
        "\n",
        "  # On tokenize en enlevant les stop_words en gardant les mentions et tweets\n",
        "  for word in text_tokenise:\n",
        "    if word.isalpha() or (word.startswith('#') and len(word_tokenize(word))>1) or (word.startswith('@') and len(word_tokenize(word))>1):\n",
        "      # We import and use the stopwords from NLTK library\n",
        "      if word.lower() not in stopwords.words('french'):\n",
        "        without_punctuation.append(word.lower())\n",
        "  dict_combine[key]['text_tokenise'] = without_punctuation"
      ],
      "metadata": {
        "id": "oID1tG1O5Ahg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_tokens(phrase_tokenisee):\n",
        "    \"\"\"\n",
        "    Fonction pour lemmatiser les tokens d'une phrase tokenisée.\n",
        "\n",
        "    :param phrase_tokenisee: Objet spaCy Doc contenant les tokens\n",
        "    :return: Liste des mots lemmatisés\n",
        "    \"\"\"\n",
        "    mots_lemmatises = []\n",
        "    global mots_lem_g  # Accès à la liste globale pour stocker tous les mots lemmatisés\n",
        "\n",
        "    j = 0\n",
        "    while j < len(phrase_tokenisee):\n",
        "        token = phrase_tokenisee[j]\n",
        "\n",
        "        if token.text.startswith('#') and j + 1 < len(phrase_tokenisee):\n",
        "            # Combinaison du hashtag avec le token suivant\n",
        "            combined_text = token.text + phrase_tokenisee[j + 1].text\n",
        "            combined_token = nlp(combined_text)\n",
        "            mots_lemmatises.append(combined_token.text)\n",
        "            mots_lem_g.append(combined_token.text)\n",
        "            j += 2  # Passer au token après le suivant\n",
        "        elif token.text.startswith('@'):\n",
        "            # Lemmatisation des mentions\n",
        "            mots_lemmatises.append(token.lemma_)\n",
        "            mots_lem_g.append(token.lemma_)\n",
        "            j += 1\n",
        "        else :\n",
        "            # Lemmatisation des autres mots\n",
        "            mots_lemmatises.append(token.lemma_)\n",
        "            mots_lem_g.append(token.lemma_)\n",
        "            j += 1\n",
        "\n",
        "    return mots_lemmatises"
      ],
      "metadata": {
        "id": "K9p7_WK_0iPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Liste globale pour stocker tous les mots lemmatisés\n",
        "mots_lem_g = []\n",
        "\n",
        "# Parcours des clés du dictionnaire\n",
        "for index, value in dict_combine.items():\n",
        "    # Création de l'objet spaCy à partir de la liste 'text_tokenise'\n",
        "    phrase_tokenisee = nlp(\" \".join(value['text_tokenise']))\n",
        "\n",
        "    # Appel de la fonction pour lemmatiser les tokens\n",
        "    value['mots_lemmatises'] = lemmatize_tokens(phrase_tokenisee)\n"
      ],
      "metadata": {
        "id": "aatI_J1k3-NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#afficher les k n-grammes les plus communs dans le texte, ici les 100 premiers trigrammes\n",
        "n=3\n",
        "k=100\n",
        "ngram = list(nltk.ngrams(mots_lem_g,n))\n",
        "fdist = FreqDist(ngram)\n",
        "print(fdist.most_common(k))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUSMRh_PaDLU",
        "outputId": "3eb33f95-a91c-4b1b-934f-584ec6629df3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('url', 'url', 'rt'), 185), (('loi', 'asil', 'immigration'), 65), (('url', 'rt', '@jlmelenchon'), 60), (('url', 'rt', '@estherbenbassa'), 59), (('#migrants', 'url', 'rt'), 46), (('url', 'rt', '@brnomorel'), 46), (('url', 'rt', '@euroecolos'), 45), (('url', 'rt', '@dversini'), 43), (('migrant', 'url', 'rt'), 39), (('loi', '#asile', 'immigration'), 31), (('rt', '@estherbenbassa', '#migrants'), 31), (('url', 'rt', '@damiencareme'), 30), (('url', 'rt', '@sandraregol'), 29), (('liberté', 'circulation', 'installation'), 28), (('politique', 'anti', 'migrant'), 25), (('url', 'rt', '@lcp'), 24), (('gt', 'gt', 'url'), 24), (('url', 'rt', '@faureolivier'), 24), (('accueil', 'digne', 'migrant'), 23), (('url', 'rt', '@mdm_france'), 23), (('url', 'rt', '@ericcoquerel'), 22), (('url', 'rt', '@manonaubryfr'), 21), (('centre', 'rétention', 'administratif'), 20), (('politique', 'migratoire', 'gouvernement'), 19), (('projet', 'loi', 'asil'), 19), (('url', 'rt', '@benoithamon'), 19), (('#lemissionpolitique', 'url', 'rt'), 19), (('#paris', 'url', 'rt'), 19), (('url', 'rt', '@franceinsoumise'), 18), (('url', 'via', '@lemondefr'), 18), (('#debatimmigration', '#pourunaccueildigne', 'url'), 18), (('lutter', 'contre', 'cause'), 17), (('viser', 'abrogation', 'délit'), 17), (('centre', 'premier', 'accueil'), 17), (('journée', 'international', 'migrant'), 16), (('abrogation', 'délit', '#solidarité'), 15), (('politique', 'migratoire', 'humaniste'), 15), (('rt', '@jlmelenchon', 'falloir'), 15), (('rt', '@euroecolos', '#newsletter'), 15), (('url', 'rt', '@emmausolidarite'), 15), (('url', 'rt', '@afpfr'), 14), (('cause', 'migration', 'forcer'), 14), (('accueil', 'migrant', 'url'), 14), (('journée', 'international', '#migrants'), 14), (('url', 'rt', '@julienbayou'), 14), (('migrant', 'url', 'url'), 14), (('gt', 'url', 'url'), 14), (('accueil', 'digne', 'url'), 13), (('url', 'politique', 'migratoire'), 13), (('politique', 'migratoire', 'européen'), 13), (('autre', 'politique', 'migratoire'), 13), (('rétention', 'administratif', 'cra'), 13), (('url', 'rt', '@yjadot'), 13), (('abonner', 'url', 'url'), 13), (('foyer', 'travailleur', 'migrant'), 13), (('migrant', 'url', 'via'), 12), (('ici', 'url', 'url'), 12), (('organiser', 'accueil', 'digne'), 12), (('url', 'rt', '@publicsenat'), 12), (('immigration', 'url', 'rt'), 12), (('#directan', 'url', 'rt'), 12), (('url', '#directan', '#loiasileimmigration'), 12), (('url', 'rt', '@bfmtv'), 12), (('url', '#migrants', '#réfugiés'), 12), (('pari', 'url', 'rt'), 12), (('url', 'rt', '@lacimade'), 12), (('loi', '#asile', '#immigration'), 12), (('@eelv', 'url', '#migrants'), 12), (('url', 'rt', '@estherbenbasser'), 12), (('lire', 'url', 'abonner'), 12), (('url', 'abonner', 'url'), 12), (('#loiasileimmigration', 'url', 'rt'), 12), (('accueillir', 'dignement', 'migrant'), 12), (('@anne_hidalgo', 'url', 'rt'), 12), (('immigration', 'exil', 'forcer'), 12), (('traiter', 'cause', 'migration'), 12), (('url', 'rt', '@aquatennen'), 12), (('circulation', 'installation', 'tout'), 11), (('url', 'aujourd', 'hui'), 11), (('venir', 'aide', 'migrant'), 11), (('liberté', 'égalité', 'fraterniter'), 11), (('url', 'via', '@libe'), 11), (('#plf2018', '#asile', '#immigration'), 11), (('#réfugiés', 'url', 'rt'), 11), (('url', 'rt', '@evajoly'), 11), (('politique', 'accueil', 'migrant'), 11), (('mettre', 'abri', 'migrant'), 11), (('url', 'rt', '@ianbrossat'), 11), (('url', 'agenda', 'public'), 11), (('bon', 'semaine', 'url'), 11), (('#migrants', 'url', '#migrants'), 10), (('accord', 'économique', 'inégal'), 10), (('#directan', '#loiasileimmigration', 'url'), 10), (('accueil', 'digne', '#migrants'), 10), (('@eelv', '@eelvidf', '@eelv_paris'), 10), (('url', 'rt', '@generationsmvt'), 10), (('url', 'rt', '@citeimmigration'), 10), (('traiter', 'libre', 'échange'), 10), (('@dversini', 'url', 'rt'), 10), (('accueil', 'url', 'rt'), 10)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for key, value in dict_combine.items():\n",
        "    tweet = value['text'].iloc[0]  # Access the tweet text\n",
        "    tokens = word_tokenize(tweet)\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    value['lemmas'] = lemmas # Add the lemmas to the dictionary"
      ],
      "metadata": {
        "id": "GvEAlr_B7mqY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "9c14f72c-51ef-4d6d-905c-f05da481675c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'iloc'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-6bb0ce6485dc>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict_combine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Access the tweet text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'iloc'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python -m spacy download fr_core_news_md\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk import FreqDist\n",
        "#spacy.load('fr_core_news_md') # No need to call spacy.load twice\n",
        "# On va lématiser les mots\n",
        "nlp = spacy.load('fr_core_news_md')\n",
        "mots_lem_g=[]\n",
        "for index in dict_combine.keys():\n",
        "  mots_lemmatises=[]\n",
        "  # Accessing the correct key 'text_tokenise'\n",
        "  phrase_de_mots_tokenise=nlp(\" \".join(dict_combine[index]['text_tokenise']))\n",
        "  contenant = {}\n",
        "  for i,w in enumerate(phrase_de_mots_tokenise):\n",
        "    contenant[i]=w\n",
        "    j=0\n",
        "  while (j)!=len(phrase_de_mots_tokenise):\n",
        "    w=contenant[j]\n",
        "    if  w.text.startswith('#') and (j+1!=len(phrase_de_mots_tokenise)):\n",
        "      w=w.text+phrase_de_mots_tokenise[j+1].text\n",
        "      w=nlp(w)  # Get the first token from the processed text\n",
        "      mots_lemmatises.append(w) # Append lemma of combined token\n",
        "      mots_lem_g.append(w)\n",
        "      j+=2\n",
        "    elif w.text.startswith('@') and (j+1!=len(phrase_de_mots_tokenise)):\n",
        "      mots_lemmatises.append(w.lemma_)\n",
        "      mots_lem_g.append(w.lemma_)\n",
        "      j+=1\n",
        "    else:\n",
        "      mots_lemmatises.append(w.lemma_)\n",
        "      mots_lem_g.append(w.lemma_)\n",
        "      j+=1\n",
        "\n",
        "  dict_combine[index]['mots_lemmatises']=mots_lemmatises"
      ],
      "metadata": {
        "id": "LN0zcFl8mIA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_pattern = re.compile(r'(https?://|www\\.)\\S+')\n",
        "pattern=r'\\w+|#[a-zA-Z0-9_àâäéèêëïîôöùûüçÀÂÄÉÈÊËÏÎÔÖÙÛÜÇ]+|@[a-zA-Z0-9_àâäéèêëïîôöùûüçÀÂÄÉÈÊËÏÎÔÖÙÛÜÇ]+|[^\\w\\s]+'\n",
        "tokenizer = RegexpTokenizer(pattern)\n",
        "for key in dict_combine.keys():\n",
        "\n",
        "  text=dict_combine[key]['text']\n",
        "  text_without_urls = url_pattern.sub('url', text) # On enlève les urls du code\n",
        "\n",
        "  # Personnalisation du tokenizer pour garder les # ou @ avec le mot\n",
        "\n",
        "  text_tokenise = tokenizer.tokenize(text_without_urls)\n",
        "  without_punctuation = []\n",
        "\n",
        "  # On tokenize en enlevant les stop_words en gardant les mentions et tweets\n",
        "  for word in text_tokenise:\n",
        "    if word.isalpha() or (word.startswith('#') and len(word_tokenize(word))>1) or (word.startswith('@') and len(word_tokenize(word))>1):\n",
        "      if word.lower() not in stopwords.words('french'):\n",
        "        without_punctuation.append(word.lower())\n",
        "  dict_combine[key]['text_tokenise'] = without_punctuation"
      ],
      "metadata": {
        "id": "jCxggiNB-Efp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#!python -m spacy download fr_core_news_md\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk import FreqDist\n",
        "#spacy.load('fr_core_news_md') # No need to call spacy.load twice\n",
        "# On va lématiser les mots\n",
        "nlp = spacy.load('fr_core_news_md')\n",
        "mots_lem_g=[]\n",
        "for index in dict_combine.keys():\n",
        "  mots_lemmatises=[]\n",
        "  # Accessing the correct key 'text_tokenise'\n",
        "  phrase_de_mots_tokenise=nlp(\" \".join(dict_combine[index]['text_tokenise']))\n",
        "  for i,w in enumerate(phrase_de_mots_tokenise):\n",
        "    if (w.text.startswith('@') or w.text.startswith('#')) and (i+1!=len(phrase_de_mots_tokenise)):\n",
        "      w=w.text+phrase_de_mots_tokenise[i+1].text\n",
        "      combined_token = nlp(w)[0]  # Get the first token from the processed text\n",
        "      mots_lemmatises.append(combined_token.lemma_) # Append lemma of combined token\n",
        "      mots_lem_g.append(combined_token.lemma_)\n",
        "    else:\n",
        "      mots_lemmatises.append(w.lemma_)\n",
        "      mots_lem_g.append(w.lemma_)\n",
        "  dict_combine[index]['mots_lemmatises']=mots_lemmatises"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fFdbfwbxYdb8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}