{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MDXxTvsFJ1X"
      },
      "source": [
        "## PART 0 : IMPORTATION OF LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "33G8T1b4x6mg"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "# Downloading necessary resources\n",
        "nltk.download('punkt')  # Download the Punkt tokenizer model\n",
        "nltk.download('stopwords')  # Download the stopwords dataset\n",
        "\n",
        "# Importing specific modules from NLTK\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Importing additional modules\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "# Downloading and loading the French spaCy language model\n",
        "!python -m spacy download fr_core_news_sm # to be executed only once\n",
        "nlp = spacy.load('fr_core_news_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcK-Xxk91Kvz"
      },
      "source": [
        "## PART 1 : IMPORTATION OF DATA AND CLEANING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "kh9etHBDxraQ"
      },
      "outputs": [],
      "source": [
        "# Define file paths for the datasets\n",
        "bdd_left_link = \"Tweet2.csv\"\n",
        "bdd_right_link = \"Tweet1.csv\"\n",
        "\n",
        "# Import the data from CSV files and skip bad lines\n",
        "tweet_df_left = pd.read_csv(bdd_left_link, on_bad_lines='skip', sep=';')\n",
        "tweet_df_right = pd.read_csv(bdd_right_link, on_bad_lines='skip', sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "Z3UZdSYh1Kv2"
      },
      "outputs": [],
      "source": [
        "tweet_df_left = tweet_df_left[[ 'data__text']].rename(columns={'data__text': 'content'})\n",
        "tweet_df_right = tweet_df_right[[ 'data__text']].rename(columns={'data__text': 'content'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "PLe7VuTI1Kv3"
      },
      "outputs": [],
      "source": [
        "# Drop data if there are duplicates or NaN tweets\n",
        "tweet_df_left=tweet_df_left.drop_duplicates(subset='content')\n",
        "tweet_df_right=tweet_df_right.drop_duplicates(subset='content')\n",
        "tweet_df_right=tweet_df_right.dropna(subset='content')\n",
        "tweet_df_left=tweet_df_left.dropna(subset='content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSemPG3T1Kv4"
      },
      "source": [
        "### Lets look at the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X22kq6fI1Kv4"
      },
      "outputs": [],
      "source": [
        "tweet_df_left"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrIA2whe1Kv4"
      },
      "outputs": [],
      "source": [
        "tweet_df_right"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1MKsMqe1Kv5"
      },
      "source": [
        "As long we can use the default index in a df, we can only keep the content of the tweet.\n",
        "\n",
        "We can also notice that there is a big difference in the number of tweets between left and right ones. Thus, we should find a way to have an unbiased model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYpwQFlUFXMj"
      },
      "source": [
        "# PART 2 : Cleaning the data (url, punctuation, lemmatisation, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('fr_core_news_sm', disable=[\"parser\", \"ner\"])\n",
        "\n",
        "# === Precompile regular expressions ===\n",
        "url_pattern = re.compile(r'(https?://|www\\.)\\S+')  # Pattern to detect URLs\n",
        "\n",
        "# === Tokenizer to handle words, hashtags, mentions, and punctuation ===\n",
        "token_pattern = r'\\w+|#[\\wàâäéèêëïîôöùûüçÀÂÄÉÈÊËÏÎÔÖÙÛÜÇ]+|@[\\wàâäéèêëïîôöùûüçÀÂÄÉÈÊËÏÎÔÖÙÛÜÇ]+|[^\\w\\s]+'\n",
        "tokenizer = RegexpTokenizer(token_pattern)\n",
        "\n",
        "# === Load French stopwords into a set for fast lookup ===\n",
        "french_stopwords = set(stopwords.words('french'))\n",
        "\n",
        "# === Initialize the French stemmer ===\n",
        "stemmer = SnowballStemmer(\"french\")\n",
        "\n",
        "\n",
        "def remove_urls(text):\n",
        "    \"\"\"Remove URLs from a given text string.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text.\n",
        "\n",
        "    Returns:\n",
        "        str: The text with URLs removed.\n",
        "    \"\"\"\n",
        "    return url_pattern.sub('', text)\n",
        "\n",
        "\n",
        "def process_text_batch_with_stemming(texts):\n",
        "    \"\"\"Process, lemmatize, and stem a batch of texts while preserving mentions and hashtags.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of text strings.\n",
        "\n",
        "    Returns:\n",
        "        list of tuples: Each tuple contains (tokens, lemmas, stems).\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Process texts in batches using spaCy to optimize performance\n",
        "    for doc in nlp.pipe(texts, batch_size=50):\n",
        "        tokens, lemmas, stems = [], [], []  # Lists to store processed words\n",
        "        hashtag = False  # Flag to track hashtags\n",
        "\n",
        "        for token in doc:\n",
        "            token_text = token.text\n",
        "\n",
        "            # Preserve mentions (@username) as they are\n",
        "            if token_text.startswith('@'):\n",
        "                tokens.append(token_text)\n",
        "                lemmas.append(token_text)\n",
        "                stems.append(token_text)  # Mentions are not stemmed\n",
        "\n",
        "            # Detect hashtags (#hashtag)\n",
        "            elif token_text.startswith('#'):\n",
        "                hashtag = True\n",
        "\n",
        "            # Process only alphabetic words\n",
        "            elif token.is_alpha:\n",
        "                word = token_text.lower()\n",
        "\n",
        "                # Ignore stopwords\n",
        "                if word not in french_stopwords:\n",
        "                    lemma = token.lemma_  # Get the lemmatized form\n",
        "\n",
        "                    # If the word was part of a hashtag, reattach '#'\n",
        "                    if hashtag:\n",
        "                        tokens.append('#' + word)\n",
        "                        lemmas.append('#' + lemma)\n",
        "                        stems.append('#' + stemmer.stem(lemma))  # Apply stemming\n",
        "                        hashtag = False  # Reset hashtag flag\n",
        "                    else:\n",
        "                        tokens.append(word)\n",
        "                        lemmas.append(lemma)\n",
        "                        stems.append(stemmer.stem(lemma))  # Apply stemming\n",
        "\n",
        "        results.append((tokens, lemmas, stems))\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "2WIdFDHyWy5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_tweets(df):\n",
        "    \"\"\"Tokenize and clean each tweet in a DataFrame.\n",
        "\n",
        "    Steps:\n",
        "    1. Remove URLs from the 'content' column.\n",
        "    2. Process texts in batches for efficiency.\n",
        "    3. Extract tokens, lemmas, and stems, and add them to the DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing a 'content' column with tweets.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Updated DataFrame with new columns ('tokens', 'lemmas', 'stem').\n",
        "    \"\"\"\n",
        "    # Step 1: Remove URLs\n",
        "    df['content_clean'] = df['content'].apply(remove_urls)\n",
        "\n",
        "    # Step 2: Process text in batches\n",
        "    processed = process_text_batch_with_stemming(df['content_clean'].tolist())\n",
        "\n",
        "    # Step 3: Unpack and store the processed data in separate DataFrame columns\n",
        "    df['tokens'], df['lemmas'], df['stem'] = zip(*processed)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "HXqnRKjDy1TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lRzh10dmeE7X"
      },
      "outputs": [],
      "source": [
        "# Tokenize and clean the tweets in the test and training datasets\n",
        "\n",
        "tokenize_tweets(tweet_df_left)\n",
        "tokenize_tweets(tweet_df_right)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(tweet_df_left['tokens'])"
      ],
      "metadata": {
        "id": "-4f1cyD1niuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV2umtyIBga-"
      },
      "outputs": [],
      "source": [
        "#print(tweet_df_left['lemmas'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(tweet_df_left['stem'])"
      ],
      "metadata": {
        "id": "YpLESXNvtMfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cells allow you to visualize how the dataset was reduced through tokenization, lemmatization, and stemming."
      ],
      "metadata": {
        "id": "Mg0IOAD60asS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def count_different_tokens (column,unique_character_set, dict_of_occurences_counter):\n",
        "#     \"\"\" count_different_tokens counts the different tokens of a dataset with the set\n",
        "#         and gives their occurences with the counter.\n",
        "\n",
        "#         Args :\n",
        "#             column (df column) : column containing the tokens\n",
        "#             unique_character_set (set) : empty set\n",
        "#             dict_of_occurences_counter (counter) : counter dictionnary to count the occurences of tokens\n",
        "#     \"\"\"\n",
        "#     for row in column :\n",
        "#       unique_character_set.update(row)\n",
        "#       dict_of_occurences_counter.update(row)\n",
        "\n",
        "#     return unique_character_set, dict_of_occurences_counter\n",
        "\n",
        "# left_df_tokens_set, left_df_lemmas_set, left_df_stem_set = set(), set(), set()\n",
        "# right_df_tokens_set, right_df_lemmas_set, right_df_stem_set = set(), set(), set()\n",
        "\n",
        "\n",
        "# left_df_tokens_counter, left_df_lemmas_counter, left_df_stem_counter = Counter(), Counter(), Counter ()\n",
        "# right_df_tokens_counter, right_df_lemmas_counter, right_df_stem_counter = Counter(), Counter(), Counter ()\n",
        "\n",
        "# count_different_tokens(tweet_df_left['tokens'], left_df_tokens_set, left_df_tokens_counter)\n",
        "# count_different_tokens(tweet_df_right['tokens'], right_df_tokens_set, right_df_tokens_counter)\n",
        "\n",
        "# count_different_tokens(tweet_df_left['lemmas'], left_df_lemmas_set, left_df_lemmas_counter)\n",
        "# count_different_tokens(tweet_df_right['lemmas'], right_df_lemmas_set, right_df_lemmas_counter)\n",
        "\n",
        "# count_different_tokens(tweet_df_left['stem'], left_df_stem_set, left_df_stem_counter)\n",
        "# count_different_tokens(tweet_df_right['stem'], right_df_stem_set, right_df_stem_counter)\n"
      ],
      "metadata": {
        "id": "PRkcy3zftrHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"Nombre de caractères uniques dans tokens left: {len(left_df_tokens_set)}\")\n",
        "# print(f\"Dictionnaire des occurrences des tokens left : {left_df_tokens_counter}\")\n",
        "# print(f\"Nombre de caractères uniques dans lemmas left: {len(left_df_lemmas_set)}\")\n",
        "# print(f\"Dictionnaire des occurrences des lemmes left: {left_df_lemmas_counter}\")\n",
        "# print(f\"Nombre de caractères uniques dans stems left: {len(left_df_stem_set)}\")\n",
        "# print(f\"Dictionnaire des occurrences des stems left: {left_df_stem_counter}\")"
      ],
      "metadata": {
        "id": "3CZZofW-yEPZ"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 : Creating the  n-grams and the combined df"
      ],
      "metadata": {
        "id": "ET3iwDBUk4is"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m58wFhcF1KwC"
      },
      "source": [
        "In order to create n-grams, you need to specify in n_list which values of n you want.\n",
        "Be careful, the model's code does not adapt to n_list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "pu404CdF5zpF"
      },
      "outputs": [],
      "source": [
        "def n_grams(list_sentence, n):\n",
        "    \"\"\"Generate n-grams from a list of words.\"\"\"\n",
        "    return list(nltk.ngrams(list_sentence, n))\n",
        "\n",
        "def generate_ngrams(df, n_list):\n",
        "    \"\"\"Create n-grams for each n in n_list and store them in new columns.\"\"\"\n",
        "    for n in n_list:\n",
        "        df[f\"{n}_grams\"] = df['stem'].apply(lambda x: n_grams(x, n))\n",
        "\n",
        "# Define n values for n-gram generation\n",
        "n_list = [2, 3, 4, 5]\n",
        "\n",
        "# Apply n-gram generation to both datasets\n",
        "generate_ngrams(tweet_df_left, n_list)\n",
        "generate_ngrams(tweet_df_right, n_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a combined DataFrame from left and right tweets\n",
        "df_combined = pd.DataFrame()\n",
        "\n",
        "# Assign labels: 1 for left tweets, 0 for right tweets\n",
        "tweet_df_left['isLeft'] = 1\n",
        "tweet_df_right['isLeft'] = 0\n",
        "\n",
        "# Concatenate both DataFrames and reset the index\n",
        "df_combined = pd.concat([tweet_df_left, tweet_df_right], ignore_index=True)\n",
        "\n",
        "df_combined"
      ],
      "metadata": {
        "id": "XBNw20GF2AC2",
        "collapsed": true
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4 : Preparing for training"
      ],
      "metadata": {
        "id": "cMKefrNq2LLo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "fT7o0Zk6HOaw"
      },
      "outputs": [],
      "source": [
        "# Split left and right DataFrames into training and test sets\n",
        "tweet_df_left_train, tweet_df_left_test = train_test_split(tweet_df_left, test_size=0.2, random_state=42)\n",
        "tweet_df_right_train, tweet_df_right_test = train_test_split(tweet_df_right, test_size=0.55, random_state=42)\n",
        "\n",
        "# Select specific columns for training and test sets\n",
        "columns_to_keep = [\"2_grams\", \"3_grams\", \"4_grams\", \"5_grams\", \"isLeft\", \"stem\"]\n",
        "\n",
        "tweet_df_left_train = tweet_df_left_train[columns_to_keep]\n",
        "tweet_df_left_test = tweet_df_left_test[columns_to_keep]\n",
        "tweet_df_right_train = tweet_df_right_train[columns_to_keep]\n",
        "tweet_df_right_test = tweet_df_right_test[columns_to_keep]\n",
        "\n",
        "# Keep only relevant columns in the combined dataset (without labels)\n",
        "df_combined = df_combined[[\"2_grams\", \"3_grams\", \"4_grams\", \"5_grams\", \"stem\"]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df_right"
      ],
      "metadata": {
        "id": "OmsN1YmG4o9k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "689cdb04-fd55-4af7-d920-6df3705a6f67"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             content  \\\n",
              "0  Dans cette vidéo, je dénonçais l'arnaque des b...   \n",
              "1  Je démonte les idées reçues sur l'immigration ...   \n",
              "2                                         data__text   \n",
              "\n",
              "                                       content_clean  \\\n",
              "0  Dans cette vidéo, je dénonçais l'arnaque des b...   \n",
              "1     Je démonte les idées reçues sur l'immigration    \n",
              "2                                         data__text   \n",
              "\n",
              "                                              tokens  \\\n",
              "0  [cette, vidéo, dénonçais, arnaque, bons, senti...   \n",
              "1              [démonte, idées, reçues, immigration]   \n",
              "2                                                 []   \n",
              "\n",
              "                                              lemmas  \\\n",
              "0  [ce, vidéo, dénoncer, arnaque, bon, sentiment,...   \n",
              "1            [démonter, idée, recevoir, immigration]   \n",
              "2                                                 []   \n",
              "\n",
              "                                                stem  \\\n",
              "0  [ce, vidéo, dénonc, arnaqu, bon, sent, mati, i...   \n",
              "1                    [démont, idé, recevoir, immigr]   \n",
              "2                                                 []   \n",
              "\n",
              "                                             2_grams  \\\n",
              "0  [(ce, vidéo), (vidéo, dénonc), (dénonc, arnaqu...   \n",
              "1  [(démont, idé), (idé, recevoir), (recevoir, im...   \n",
              "2                                                 []   \n",
              "\n",
              "                                             3_grams  \\\n",
              "0  [(ce, vidéo, dénonc), (vidéo, dénonc, arnaqu),...   \n",
              "1  [(démont, idé, recevoir), (idé, recevoir, immi...   \n",
              "2                                                 []   \n",
              "\n",
              "                                             4_grams  \\\n",
              "0  [(ce, vidéo, dénonc, arnaqu), (vidéo, dénonc, ...   \n",
              "1                  [(démont, idé, recevoir, immigr)]   \n",
              "2                                                 []   \n",
              "\n",
              "                                             5_grams  isLeft  \n",
              "0  [(ce, vidéo, dénonc, arnaqu, bon), (vidéo, dén...       0  \n",
              "1                                                 []       0  \n",
              "2                                                 []       0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e63601d6-0e17-4b10-97f8-c2a55aa11beb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>content_clean</th>\n",
              "      <th>tokens</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>stem</th>\n",
              "      <th>2_grams</th>\n",
              "      <th>3_grams</th>\n",
              "      <th>4_grams</th>\n",
              "      <th>5_grams</th>\n",
              "      <th>isLeft</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dans cette vidéo, je dénonçais l'arnaque des b...</td>\n",
              "      <td>Dans cette vidéo, je dénonçais l'arnaque des b...</td>\n",
              "      <td>[cette, vidéo, dénonçais, arnaque, bons, senti...</td>\n",
              "      <td>[ce, vidéo, dénoncer, arnaque, bon, sentiment,...</td>\n",
              "      <td>[ce, vidéo, dénonc, arnaqu, bon, sent, mati, i...</td>\n",
              "      <td>[(ce, vidéo), (vidéo, dénonc), (dénonc, arnaqu...</td>\n",
              "      <td>[(ce, vidéo, dénonc), (vidéo, dénonc, arnaqu),...</td>\n",
              "      <td>[(ce, vidéo, dénonc, arnaqu), (vidéo, dénonc, ...</td>\n",
              "      <td>[(ce, vidéo, dénonc, arnaqu, bon), (vidéo, dén...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Je démonte les idées reçues sur l'immigration ...</td>\n",
              "      <td>Je démonte les idées reçues sur l'immigration</td>\n",
              "      <td>[démonte, idées, reçues, immigration]</td>\n",
              "      <td>[démonter, idée, recevoir, immigration]</td>\n",
              "      <td>[démont, idé, recevoir, immigr]</td>\n",
              "      <td>[(démont, idé), (idé, recevoir), (recevoir, im...</td>\n",
              "      <td>[(démont, idé, recevoir), (idé, recevoir, immi...</td>\n",
              "      <td>[(démont, idé, recevoir, immigr)]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data__text</td>\n",
              "      <td>data__text</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e63601d6-0e17-4b10-97f8-c2a55aa11beb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e63601d6-0e17-4b10-97f8-c2a55aa11beb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e63601d6-0e17-4b10-97f8-c2a55aa11beb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5246f9db-9b1f-4e2a-a78a-4427b1057642\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5246f9db-9b1f-4e2a-a78a-4427b1057642')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5246f9db-9b1f-4e2a-a78a-4427b1057642 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "tweet_df_right",
              "summary": "{\n  \"name\": \"tweet_df_right\",\n  \"rows\": 11194,\n  \"fields\": [\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11194,\n        \"samples\": [\n          \"Pr\\u00e8s d\\u2019un milliard d\\u2019euros d\\u00e9vers\\u00e9s pour soigner les clandestins pr\\u00e9sents sur notre territoire, pendant que des Fran\\u00e7ais peinent \\u00e0 se soigner correctement.  \\\\n Il est urgent de stopper cette pompe aspirante et que l\\u2019argent soit destin\\u00e9 aux n\\u00f4tres ! \\\\n https://t.co/SLlJXjoZwa\",\n          \".@nicolasbayfn : \\u00ab\\u00a0Macron n\\u2019a pas le courage de s\\u2019attaquer aux gisements d\\u2019\\u00e9conomies que repr\\u00e9sente la lutte contre la fraude sociale et fiscale, contre l\\u2019immigration.\\u00a0\\u00bb @BFMTV #TF1EMacron\",\n          \"RT @lafrancelibretv: \\ud83c\\udf99Pour la d\\u00e9put\\u00e9e Val\\u00e9rie Boyer : \\\"La mise en sc\\u00e8ne d'Emmanuel Macron ab\\u00eeme la fonction\\\" \\\\n  \\\\n Retrouvez l'entretien de @valerieboyer13 avec JB Roqu\\u00e8s sur \\ud83d\\udcfa @lafrancelibretv \\ud83d\\udcbb \\\\n  \\\\n Au menu : \\\\n #Calmels #Aquarius #immigration #UE #islamisme #bioethique #Macron \\\\n  \\\\n \\u2935\\ufe0f \\\\n https://t.co/F6Qq7CrSMm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content_clean\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11188,\n        \"samples\": [\n          \"RT @_AvecMarion: .@Marion_M_Le_Pen: \\\"La submersion migratoire n'est certainement pas une chance pour le peuple fran\\u00e7ais\\\"\\\\n#UDTFN\",\n          \"RT @Stephane_Ravier: Le Rassemblement National des Bouches-du-Rh\\u00f4ne mobilis\\u00e9 sur le port de #Marseille contre l\\u2019#Aquarius : ras-le-bol de ce trafic humain des ONG complices des passeurs ! L\\u2019Etat doit confisquer ce bateau et emp\\u00eacher la submersion migratoire une bonne fois pour toutes ! \",\n          \"Aujourd'hui, d\\u00e9fendre les femmes, c'est les prot\\u00e9ger contre l'immigration islamique. C'est le plus grand danger qui menace les femmes. \\\\n  \\\\n #FaceAFace\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemmas\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stem\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"2_grams\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"3_grams\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"4_grams\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"5_grams\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"isLeft\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modèles"
      ],
      "metadata": {
        "id": "5dU5xubUy3EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tuple_to_string(list_of_tuples):\n",
        "    \"\"\"Convert a list of n-grams (tuples) into a single space-separated string.\"\"\"\n",
        "    return ' '.join(['_'.join(t) for t in list_of_tuples])\n",
        "\n",
        "# Apply function to convert n-grams to string format for each dataset\n",
        "for df in [tweet_df_left_train, tweet_df_left_test, tweet_df_right_train, tweet_df_right_test]:\n",
        "    df[\"total_n_grams\"] = df[[\"2_grams\", \"3_grams\", \"4_grams\", \"5_grams\"]].apply(\n",
        "        lambda row: ' '.join(row.apply(tuple_to_string)), axis=1\n",
        "    )\n",
        "\n",
        "# Concatenate left and right datasets to form train and test sets\n",
        "df_train = pd.concat([tweet_df_left_train, tweet_df_right_train], ignore_index=True)\n",
        "df_test = pd.concat([tweet_df_left_test, tweet_df_right_test], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "L2RzlAI5r-2p"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head(3)"
      ],
      "metadata": {
        "id": "o29QT1-J3UEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "9Mtn6q4P4XrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "ZtV30hQ_4ItN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
        "X_train = vectorizer.fit_transform(df_train[\"total_n_grams\"])\n",
        "y_train = df_train[\"isLeft\"]\n",
        "X_test = vectorizer.transform(df_test[\"total_n_grams\"])\n",
        "y_test = df_test[\"isLeft\"]"
      ],
      "metadata": {
        "id": "IgLpNAcrxOVQ"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation du modèle\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Entraînement du modèle\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "p-GW5soqzquo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ypred = model.predict(X_test)\n",
        "print(accuracy_score(y_test,ypred))"
      ],
      "metadata": {
        "id": "REg9qMOt1DkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN"
      ],
      "metadata": {
        "id": "rpAJCsMdi6Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "SWDObtbdjFjh"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the maximum row length and unique words in 'stem' column to define the RNN parameters\n",
        "max_length = 0\n",
        "unique_words = set()\n",
        "\n",
        "for row in df_train['stem']:\n",
        "    max_length = max(len(row), max_length)\n",
        "    unique_words.update(row)\n",
        "\n",
        "# Print results\n",
        "print(f\"Number of unique words: {len(unique_words)}\")\n",
        "print(f\"Maximum row length: {max_length}\")\n"
      ],
      "metadata": {
        "id": "0POo72MPjOpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define vocabulary size, maximum sequence length, and embedding dimension\n",
        "max_vocab_size = len(unique_words)\n",
        "max_sequence_length = max_length\n",
        "embedding_dim = 100"
      ],
      "metadata": {
        "id": "VTXuA836kIns"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and fit the tokenizer on the training data\n",
        "tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<OOV>\")  # Handle out-of-vocabulary words\n",
        "tokenizer.fit_on_texts(df_train['stem'])\n",
        "\n",
        "# Convert text to sequences\n",
        "X_train_seq = tokenizer.texts_to_sequences(df_train['stem'])\n",
        "X_test_seq  = tokenizer.texts_to_sequences(df_test['stem'])\n",
        "\n",
        "# Pad sequences to ensure uniform input size\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "X_test_pad  = pad_sequences(X_test_seq, maxlen=max_sequence_length, padding='post', truncating='post')\n"
      ],
      "metadata": {
        "id": "4qBtEbo-lUQY"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Build the RNN Model ===\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding layer to convert word indices into dense vectors\n",
        "model.add(Embedding(input_dim=max_vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "\n",
        "# Bidirectional LSTM to capture dependencies in both directions\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "\n",
        "# Dropout layer to prevent overfitting\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output layer with sigmoid activation for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# === Compile the Model ===\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "# Reduce learning rate when validation loss stops improving\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Display model architecture\n",
        "model.summary()\n",
        "\n",
        "# Early stopping to prevent unnecessary training when validation loss stops improving\n",
        "#early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True, verbose=1)"
      ],
      "metadata": {
        "id": "kMP92E2VmyIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training of the model\n",
        "history = model.fit(X_train_pad, y_train, batch_size=32, epochs=12, validation_split=0.2, callbacks=[lr_scheduler])"
      ],
      "metadata": {
        "id": "dy3Aq0f-nAui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluation du modèle sur le jeu de test\n",
        "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "w2v41bVAnXmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM's solution"
      ],
      "metadata": {
        "id": "GBpL1vlD6c1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT"
      ],
      "metadata": {
        "id": "a5uzVo_nchU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ChatGpt LinearRegression"
      ],
      "metadata": {
        "id": "RGcXc0SO5Ti-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "###############################################################################\n",
        "# 1) Exemples de fonctions de transformation pour fusionner 2_grams et 3_grams\n",
        "###############################################################################\n",
        "def convert_ngrams_to_string(row):\n",
        "    \"\"\"\n",
        "    Convertit la liste de tuples (2_grams) et la liste de tuples (3_grams)\n",
        "    en une unique chaîne de caractères, pour pouvoir utiliser CountVectorizer.\n",
        "    \"\"\"\n",
        "    # row[\"2_grams\"] est une liste de tuples (ex: [('avec','yoyo'), ('je','mange'), ...])\n",
        "    # On va transformer chaque tuple ('avec','yoyo') en 'avec_yoyo'\n",
        "    two_grams_str  = [\"_\".join(tup) for tup in row[\"2_grams\"]]\n",
        "    three_grams_str = [\"_\".join(tup) for tup in row[\"3_grams\"]]\n",
        "\n",
        "    # On fusionne les deux listes sous forme d'une seule chaîne\n",
        "    # Exemple : \"avec_yoyo je_mange ... je_mange_en classe_de_francaise ...\"\n",
        "    combined_str = \" \".join(two_grams_str + three_grams_str)\n",
        "    return combined_str\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 2) Préparation des données d'entraînement et de test\n",
        "###############################################################################\n",
        "# Supposons que vos dataframes soient déjà définis :\n",
        "#   tweet_df_left_train, tweet_df_left_test,\n",
        "#   tweet_df_right_train, tweet_df_right_test\n",
        "#\n",
        "# et qu'ils ont chacun les colonnes : [\"2_grams\", \"3_grams\", \"isLeft\"].\n",
        "\n",
        "# Optionnellement, vous pouvez concaténer left et right si vous le souhaitez :\n",
        "df_train = pd.concat([tweet_df_left_train, tweet_df_right_train], ignore_index=True)\n",
        "df_test  = pd.concat([tweet_df_left_test, tweet_df_right_test], ignore_index=True)\n",
        "\n",
        "# On applique la fonction de conversion pour créer une nouvelle colonne textuelle\n",
        "df_train[\"text_ngrams\"] = df_train.apply(convert_ngrams_to_string, axis=1)\n",
        "df_test[\"text_ngrams\"]  = df_test.apply(convert_ngrams_to_string, axis=1)\n",
        "\n",
        "# X et y pour l'entraînement\n",
        "X_train_text = df_train[\"text_ngrams\"]  # données textuelles vectorisables\n",
        "y_train = df_train[\"isLeft\"]            # cible binaire (0 ou 1)\n",
        "\n",
        "# X et y pour le test\n",
        "X_test_text = df_test[\"text_ngrams\"]\n",
        "y_test = df_test[\"isLeft\"]\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 3) Vectorisation (CountVectorizer) + Entraînement du modèle (LinearRegression)\n",
        "###############################################################################\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train_text)\n",
        "X_test_vec  = vectorizer.transform(X_test_text)\n",
        "\n",
        "# Instanciation et entraînement de la régression linéaire\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "###############################################################################\n",
        "# 4) Prédictions et évaluation\n",
        "###############################################################################\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# Pour la régression linéaire, calculons MSE et R²\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2  = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error : {mse:.4f}\")\n",
        "print(f\"R^2 Score          : {r2:.4f}\")\n",
        "\n",
        "# Si vous souhaitez convertir la prédiction en 0/1, vous pouvez faire\n",
        "# par exemple un seuil à 0.5 :\n",
        "y_pred_class = [1 if val >= 0.5 else 0 for val in y_pred]\n",
        "\n",
        "# Taux de bonne classification (accuracy) éventuelle si vous voulez un score simple :\n",
        "accuracy = sum(y_pred_class == y_test) / len(y_test)\n",
        "print(f\"Accuracy (avec seuil 0.5) : {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "C8_ngiTm5EOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_set=set()\n",
        "for row in df_test['2_grams']:\n",
        "  for n_gram in row:\n",
        "    count_set.add(n_gram)\n",
        "\n",
        "for row in df_test['3_grams']:\n",
        "  for n_gram in row:\n",
        "    count_set.add(n_gram)\n",
        "\n",
        "print(len(count_set))"
      ],
      "metadata": {
        "id": "gj_UvifQC79u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ChatGpt LogisticRegression"
      ],
      "metadata": {
        "id": "czIWHKov7G28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "###############################################################################\n",
        "# 1) Exemple de fonction de transformation : listes de tuples -> chaîne de caractères\n",
        "###############################################################################\n",
        "def convert_ngrams_to_string(row):\n",
        "    \"\"\"\n",
        "    Convertit la liste de tuples (2_grams) et la liste de tuples (3_grams)\n",
        "    en une unique chaîne de caractères, pour pouvoir utiliser CountVectorizer.\n",
        "    \"\"\"\n",
        "    # row[\"2_grams\"] est une liste de tuples (ex: [('avec','yoyo'), ('je','mange'), ...])\n",
        "    # row[\"3_grams\"] est également une liste de tuples\n",
        "    two_grams_str  = [\"_\".join(tup) for tup in row[\"2_grams\"]]\n",
        "    three_grams_str = [\"_\".join(tup) for tup in row[\"3_grams\"]]\n",
        "\n",
        "    # On fusionne les deux listes sous forme d'une seule chaîne\n",
        "    combined_str = \" \".join(two_grams_str + three_grams_str)\n",
        "    return combined_str\n",
        "\n",
        "###############################################################################\n",
        "# 2) Supposons que vos DataFrames soient déjà chargés :\n",
        "#    tweet_df_left_train, tweet_df_left_test, tweet_df_right_train, tweet_df_right_test\n",
        "#    chacun ayant [\"2_grams\", \"3_grams\", \"isLeft\"] comme colonnes.\n",
        "###############################################################################\n",
        "# Exemple : On les combine pour obtenir un dataset global de train / test\n",
        "\n",
        "# On fusionne les dataframes pour l'entraînement\n",
        "df_train = pd.concat([tweet_df_left_train, tweet_df_right_train], ignore_index=True)\n",
        "df_test  = pd.concat([tweet_df_left_test, tweet_df_right_test], ignore_index=True)\n",
        "\n",
        "# Application de la fonction de conversion pour créer une colonne textuelle\n",
        "df_train[\"text_ngrams\"] = df_train.apply(convert_ngrams_to_string, axis=1)\n",
        "df_test[\"text_ngrams\"]  = df_test.apply(convert_ngrams_to_string, axis=1)\n",
        "\n",
        "# Séparation X / y pour l'entraînement\n",
        "X_train = df_train[\"text_ngrams\"]\n",
        "y_train = df_train[\"isLeft\"]  # binaire : 0 ou 1\n",
        "\n",
        "# Séparation X / y pour le test\n",
        "X_test = df_test[\"text_ngrams\"]\n",
        "y_test = df_test[\"isLeft\"]\n",
        "\n",
        "###############################################################################\n",
        "# 3) Vectorisation + Entraînement du modèle (LogisticRegression)\n",
        "###############################################################################\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec  = vectorizer.transform(X_test)\n",
        "\n",
        "# Instanciation de la régression logistique\n",
        "model = LogisticRegression(max_iter=1000)  # on peut augmenter max_iter si besoin\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "###############################################################################\n",
        "# 4) Prédiction et évaluation\n",
        "###############################################################################\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# Calcul de l'accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy : {accuracy:.4f}\")\n",
        "\n",
        "# Matrice de confusion\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matrice de confusion :\")\n",
        "print(cm)\n",
        "\n",
        "# Rapport de classification (précision, rappel, f1-score)\n",
        "print(\"Rapport de classification :\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "jMp_SO3mR_Ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deepseek"
      ],
      "metadata": {
        "id": "NzreK1q8clJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deepseek regression"
      ],
      "metadata": {
        "id": "mUv5OxzsR1zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Fonction pour convertir les tuples en chaînes\n",
        "def process_ngram_column(df, column_name):\n",
        "    return df[column_name].apply(\n",
        "        lambda x: ['_'.join(logram) for gram in x] if isinstance(x, list) else []\n",
        "    )\n",
        "\n",
        "# Appliquer le prétraitement sur toutes les DataFrames\n",
        "for df in [tweet_df_left_train, tweet_df_left_test, tweet_df_right_train, tweet_df_right_test]:\n",
        "    df[\"2_grams_str\"] = process_ngram_column(df, \"2_grams\")\n",
        "    df[\"3_grams_str\"] = process_ngram_column(df, \"3_grams\")\n",
        "    df[\"all_ngrams\"] = df[\"2_grams_str\"] + df[\"3_grams_str\"]\n",
        "\n",
        "# Combiner les données d'entraînement\n",
        "train_data = pd.concat([tweet_df_left_train, tweet_df_right_train])\n",
        "test_data = pd.concat([tweet_df_left_test, tweet_df_right_test])\n",
        "\n",
        "# Créer un texte unique par tweet\n",
        "train_data[\"text\"] = train_data[\"all_ngrams\"].apply(lambda x: \" \".join(x))\n",
        "test_data[\"text\"] = test_data[\"all_ngrams\"].apply(lambda x: \" \".join(x))\n",
        "\n",
        "# Vectoriser avec CountVectorizer\n",
        "vectorizer = CountVectorizer(max_features=500000)  # Limiter les features pour éviter l'explosion dimensionnelle\n",
        "X_train = vectorizer.fit_transform(train_data[\"text\"])\n",
        "X_test = vectorizer.transform(test_data[\"text\"])\n",
        "y_train = train_data[\"isLeft\"]\n",
        "y_test = test_data[\"isLeft\"]\n",
        "\n",
        "# Entraîner la régression linéaire\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Prédiction et évaluation\n",
        "predictions = model.predict(X_test)\n",
        "print(\"Coefficients importants :\", dict(zip(vectorizer.get_feature_names_out(), model.coef_.round(2))))\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "print(f\"Mean Squared Error: {mean_squared_error(y_test, predictions):.3f}\")\n",
        "print(f\"R² Score: {r2_score(y_test, predictions):.3f}\")\n",
        "\n",
        "y_pred_class = [1 if val >= 0.5 else 0 for val in predictions]\n",
        "accuracy = sum(y_pred_class == y_test) / len(y_test)\n",
        "print(f\"Accuracy (avec seuil 0.5) : {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "sva0ixjiz6AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DeepSeek classification"
      ],
      "metadata": {
        "id": "K92um8w6RyZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1. Fonction de prétraitement des n-grams\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "def process_ngrams(df):\n",
        "    # Copie du dataframe pour éviter les modifications inplace\n",
        "    df = df.copy()\n",
        "\n",
        "    # Conversion des tuples en chaînes de caractères\n",
        "    for col in ['2_grams', '3_grams']:\n",
        "        df[col] = df[col].apply(\n",
        "            lambda x: ['_'.join(gram) for gram in x] if isinstance(x, list) else []\n",
        "        )\n",
        "\n",
        "    # Combinaison de tous les n-grams en une seule chaîne\n",
        "    df['all_ngrams'] = df['2_grams'] + df['3_grams']\n",
        "    df['text'] = df['all_ngrams'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    return df\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2. Application du prétraitement sur tous les datasets\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "# Traitement des données d'entraînement\n",
        "train_left = process_ngrams(tweet_df_left_train)\n",
        "train_right = process_ngrams(tweet_df_right_train)\n",
        "train_data = pd.concat([train_left, train_right], axis=0)\n",
        "\n",
        "# Traitement des données de test\n",
        "test_left = process_ngrams(tweet_df_left_test)\n",
        "test_right = process_ngrams(tweet_df_right_test)\n",
        "test_data = pd.concat([test_left, test_right], axis=0)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3. Vectorisation des caractéristiques\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    max_features=1000000,  # Limite le nombre de features\n",
        "    ngram_range=(1, 1)  # Conserve les n-grams individuels\n",
        ")\n",
        "\n",
        "X_train = vectorizer.fit_transform(train_data['text'])\n",
        "X_test = vectorizer.transform(test_data['text'])\n",
        "y_train = train_data['isLeft']\n",
        "y_test = test_data['isLeft']\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4. Entraînement du modèle de classification\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "model = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    class_weight='balanced',  # Gère les déséquilibres de classes\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 5. Évaluation du modèle\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "# Prédictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Métriques\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.3f}\")\n",
        "print(\"\\nMatrice de confusion:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 6. Analyse des features importantes (optionnel)\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "# Récupération des coefficients\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "coefs = model.coef_[0]\n",
        "\n",
        "# Création d'un dataframe d'analyse\n",
        "coef_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'coefficient': coefs,\n",
        "    'abs_coef': abs(coefs)\n",
        "})\n",
        "\n",
        "# Top 10 des features les plus importantes pour chaque classe\n",
        "print(\"\\nTop 10 features pro-Left:\")\n",
        "print(coef_df.sort_values('coefficient', ascending=False).head(10)[['feature', 'coefficient']])\n",
        "\n",
        "print(\"\\nTop 10 features pro-Right:\")\n",
        "print(coef_df.sort_values('coefficient', ascending=True).head(10)[['feature', 'coefficient']])"
      ],
      "metadata": {
        "id": "95uCxB9hRvxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ressources"
      ],
      "metadata": {
        "id": "FUImMAiXoThk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download fr_core_news_sm # Download the 'fr_core_news_sm' model\n",
        "nlp = spacy.load('fr_core_news_sm')\n",
        "\n",
        "# Precompile the module regexp for urls\n",
        "url_pattern = re.compile(r'(https?://|www\\.)\\S+')\n",
        "\n",
        "# Define a tokenizer for word, hastags, @ and punctuation\n",
        "pattern = r'\\w+|#[\\wàâäéèêëïîôöùûüçÀÂÄÉÈÊËÏÎÔÖÙÛÜÇ]+|@[\\wàâäéèêëïîôöùûüçÀÂÄÉÈÊËÏÎÔÖÙÛÜÇ]+|[^\\w\\s]+'\n",
        "tokenizer = RegexpTokenizer(pattern)\n",
        "\n",
        "# Load french stop words\n",
        "french_stopwords = set(stopwords.words('french'))\n",
        "\n",
        "def remove_urls(text):\n",
        "    \"\"\"Delete the urls of the text\"\"\"\n",
        "    return url_pattern.sub('', text)\n",
        "\n",
        "def lemmatize (word_lowered):\n",
        "   if word_lowered not in french_stopwords:\n",
        "     filtered=word_lowered\n",
        "     if not word_lowered.startswith(('#', '@')):\n",
        "        return (filtered, nlp(word_lowered)[0].lemma_)  # If the word is not a mention or topic we lemmatize\n",
        "     else:\n",
        "        return (filtered, word_lowered)\n",
        "\n",
        "def remove_and_lemmatize (text):\n",
        "    \"\"\"Delete punctuation and stopwords, then tokenize and lemmatize .\n",
        "       Words starting with # and @ are unchanged.\"\"\"\n",
        "    tokens = tokenizer.tokenize(text)                             # Tokenize the text with our regexp\n",
        "    filtered = []\n",
        "    lemmatized = []\n",
        "    for word in tokens:\n",
        "        if word.isalpha() or word.startswith(('#', '@')):         # If it is a word, mention or topic (@ or #)\n",
        "          word_lowered = word.lower()\n",
        "          a = lemmatize(word_lowered)\n",
        "          if a is not None :\n",
        "            word_filtered, word_lemmatized = a    # If it is, we just add it without lemmatizing\n",
        "            filtered.append(word_filtered)\n",
        "            lemmatized.append(word_lemmatized)\n",
        "    return filtered, lemmatized\n",
        "\n",
        "def tokenize_tweets(df):\n",
        "    \"\"\"Tokeniser et nettoyer chaque tweet dans le DataFrame.\"\"\"\n",
        "    # We remove urls, punctuation and stopwords\n",
        "    df['clean_content'] = df['content'].apply(remove_urls).apply(remove_and_lemmatize)\n",
        "    # Creat columns for tokens and words lemmatized\n",
        "    df[['tokens', 'lemmas']] = pd.DataFrame(df['clean_content'].tolist(), index=df.index)"
      ],
      "metadata": {
        "id": "v3NXBsuw3AZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RtgBG-Vx1EH"
      },
      "outputs": [],
      "source": [
        "# Precompile URL regex pattern\n",
        "url_pattern = re.compile(r'(https?://|www\\.)\\S+')\n",
        "\n",
        "# Define tokenizer pattern for words, hashtags, mentions, and punctuation\n",
        "pattern = r'\\w+|#[\\wàâäéèêëïîôöùûüçÀÂÄÉÈÊËÏÎÔÖÙÛÜÇ]+|@[\\wàâäéèêëïîôöùûüçÀÂÄÉÈÊËÏÎÔÖÙÛÜÇ]+|[^\\w\\s]+'\n",
        "tokenizer = RegexpTokenizer(pattern)\n",
        "\n",
        "# Load French stopwords once and convert to a set for faster lookup\n",
        "french_stopwords = set(stopwords.words('french'))\n",
        "\n",
        "def remove_urls(text):\n",
        "    \"\"\"Remove URLs from strings.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text.\n",
        "\n",
        "    Returns:\n",
        "        str: The text with URLs removed.\n",
        "    \"\"\"\n",
        "    return url_pattern.sub('', text)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"Remove punctuation and stopwords, then tokenize and lemmatize.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be tokenized and cleaned.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two lists:\n",
        "            - A list of filtered tokens (words).\n",
        "            - A list of lemmatized words.\n",
        "    \"\"\"\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    global mots_lem_g\n",
        "    filtered = []\n",
        "    lemmatized = []\n",
        "    for word in tokens:\n",
        "        if word.isalpha() or word.startswith(('#', '@')):  # Check if it's a word or starts with '#' or '@'\n",
        "            word_lower = word.lower()\n",
        "            if word_lower not in french_stopwords:  # Remove stopwords\n",
        "                filtered.append(word_lower)\n",
        "                if not word_lower.startswith(('#', '@')):  # Lemmatize non-hashtags/mentions\n",
        "                    lemmatized.append(nlp(word_lower)[0].lemma_)\n",
        "                else:\n",
        "                    lemmatized.append(word_lower)  # Keep hashtags and mentions unchanged\n",
        "    mots_lem_g.extend(lemmatized)  # Add lemmatized words to the global list\n",
        "    return filtered, lemmatized\n",
        "\n",
        "def tokenize_tweets(tweets):\n",
        "    \"\"\"Tokenize and clean each tweet in the dictionary.\n",
        "\n",
        "    Args:\n",
        "        tweets (dict): A dictionary where keys are tweet indices and values are tweet data (including text).\n",
        "\n",
        "    Returns:\n",
        "        None: This function updates the 'tweets' dictionary in place with tokenized and lemmatized text.\n",
        "    \"\"\"\n",
        "    for tweet in tweets.values():\n",
        "        text = tweet.get('text', '')  # Get tweet text\n",
        "        text_no_urls = remove_urls(text)  # Remove URLs\n",
        "        tokens, lemmas = remove_punctuation(text_no_urls)  # Remove punctuation and lemmatize\n",
        "        tweet['text_tokenise'] = tokens  # Store tokens\n",
        "        tweet['mots_lemmatises'] = lemmas  # Store lemmatized words\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}